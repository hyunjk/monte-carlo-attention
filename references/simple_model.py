# -*- coding: utf-8 -*-
"""hw6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jbFid4tBzIOj51PP_-23wqCXnO8bFUuL
"""

import math
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
from tqdm import tqdm
from sklearn.metrics import f1_score, accuracy_score

import utils

FLAG = False


# Fast Monte-Carlo Approximation of Attention Mechanism

class Hyperparameter:
    def __init__(self, **entries):
        self.__dict__.update(entries)


class Config:
    def __init__(self, num_layers, num_heads, head_dim, hidden_dim, dropout_p):
        self.num_layers = 4

        self.num_heads = 4
        self.head_dim = 32
        self.hidden_dim = 32
        self.dropout_p = 0.3

        self.feature_dim = self.head_dim * self.num_heads


class MyModel(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config

        self.static_embedding = nn.Linear(7, config.feature_dim)
        self.dynamic_embedding = nn.Linear(35, config.feature_dim)
        self.time_embedding = nn.Embedding(72, config.feature_dim)

        self.transformer = Transformer(config)

        self.classifier = nn.Linear(config.feature_dim, 2)

    def forward(self, static_feature, dynamic_feature):
        #  Position (time) embeddings
        seq = torch.arange(dynamic_feature.size(1)).expand(dynamic_feature.shape[:2]).cuda()
        time_emb = self.time_embedding(seq)

        x_sta = self.static_embedding(static_feature)
        x_dyn = self.dynamic_embedding(dynamic_feature) + time_emb

        x = torch.cat((x_sta.unsqueeze(1), x_dyn), dim=1)
        y = self.transformer(x)

        # use the first feature for classification
        z = y[:, 0, :]
        logits = self.classifier(z)

        return logits


class Transformer(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.config = config
        self.layers = []
        for i in range(self.config.num_layers):
            layer = TransformerLayer(config)
            self.add_module(f'Layer{i}', layer)
            self.layers.append(layer)

    def forward(self, x):
        # Recurrent layer structure like in ALBERT
        for i in range(self.config.num_layers):
            x = self.layers[i](x)

        return x


class RecurrentTransformer(nn.Module):

    def __init__(self, config):
        super().__init__()

        self.config = config
        self.layer = TransformerLayer(config)

    def forward(self, x):
        # Recurrent layer structure like in ALBERT
        for i in range(self.config.num_layers):
            x = self.layer(x)

        return x


class TransformerLayer(nn.Module):

    def __init__(self, config):
        super().__init__()

        self.attn = MultiheadAttention(config)
        self.ffn = nn.Sequential(
            nn.Linear(config.feature_dim, config.hidden_dim),
            nn.ReLU(),
            nn.Linear(config.hidden_dim, config.feature_dim),
            nn.Dropout(config.dropout_p)
        )
        self.norm = nn.LayerNorm(config.feature_dim)

    def forward(self, x):
        y = self.attn(x)
        z = self.ffn(y)
        return self.norm(y + z)


class MultiheadAttention(nn.Module):

    def __init__(self, config):
        super().__init__()

        self.config = config

        self.key_proj = nn.Linear(config.feature_dim, config.feature_dim)
        self.query_proj = nn.Linear(config.feature_dim, config.feature_dim)
        self.value_proj = nn.Linear(config.feature_dim, config.feature_dim)

        self.attn_softmax = nn.Softmax(-1)
        self.dense = nn.Linear(config.feature_dim, config.feature_dim)
        self.norm = nn.LayerNorm(config.feature_dim)
        self.dropout = nn.Dropout(config.dropout_p)

        self.rand_mode = False

    def forward(self, x):
        # (N, L, E) -> (N, L, head_dim * num_heads)
        query = self.query_proj(x)
        key = self.key_proj(x)

        # Separate heads
        # (N, L, num_heads * head_dim) -> (N, num_heads, L, head_dim)
        query = self._separate_heads(query)
        key = self._separate_heads(key)

        # Calculate the attention scores
        # (N, num_heads, L, head_dim) * (N, num_head, head_dim, L) -> (N, num_head, L, L)
        attn_vv = torch.matmul(query, torch.transpose(key, -1, -2)) / math.sqrt(self.config.head_dim)

        # Apply softmax to attention weights
        attn = self.attn_softmax(attn_vv)
        global FLAG

        if FLAG:

            # remove batch
            attn = torch.squeeze(attn, dim=0)
            x = torch.squeeze(x, dim=0)

            attn_value = utils.multi_rand_attn_cuda(attn, x, self.value_proj.weight)

            # add virtual batch of size one
            attn_value = torch.unsqueeze(attn_value, dim=0)

        else:
            value = self.value_proj(x)
            value = self._separate_heads(value)

            # Applying attention weights
            # (N, num_heads, L, L) * (N, num_heads, L, head_dim) -> (N, num_heads, L, head_dim)
            attn_value = torch.matmul(attn, value)

            # Merge heads
            # (N, num_heads, L, head_dim) -> (N, L, num_heads * head_dim)
            attn_value = self._merge_heads(attn_value)

        # (Optional) dropout
        # attn_value = self.dropout(attn_value)

        y = self.dense(attn_value)

        return self.norm(x + y)

    # (N, L, num_heads * head_dim) -> (N, num_heads, L, head_dim)
    def _separate_heads(self, x):
        n, l, _ = x.shape
        y = x.reshape(n, l, self.config.num_heads, self.config.head_dim)
        return y.permute(0, 2, 1, 3)

    # (N, num_heads, L, head_dim) -> (N, L, num_heads * head_dim)
    def _merge_heads(self, x):
        n, _, l, _ = x.shape
        y = x.permute(0, 2, 1, 3)
        return y.reshape(n, l, self.config.num_heads * self.config.head_dim)


class Dataset(nn.Module):

    def __init__(self, static_data_path, time_series_data_path, label_path):
        super().__init__()

        self.static_data = torch.load(static_data_path)
        self.time_series_data = torch.load(time_series_data_path)
        self.labels = torch.load(label_path)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, item):
        return self.static_data[item], self.time_series_data[item], self.labels[item].long()


class Trainer:

    def __init__(self, args):
        self.args = args

        config = Config(args.num_layers, args.num_heads, args.head_dim, args.hidden_dim, args.dropout_p)

        self.model = MyModel(config).cuda()

        self.num_params = sum([p.numel() for p in self.model.parameters() if p.requires_grad])

        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=args.lr)

        self.lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer,
                                                        lr_lambda=lambda epoch: args.lr_lambda ** epoch)
        self.global_steps = 0

    def train(self, dataset_train, dataset_valid, dataset_test):

        loader_train = data.DataLoader(dataset_train, self.args.batch_size, shuffle=True)
        loader_valid = data.DataLoader(dataset_valid, 1, shuffle=True)
        loader_test = data.DataLoader(dataset_test, 1, shuffle=True)

        self.global_steps = 0

        for epoch in range(self.args.num_epochs):
            print(f"Epoch {epoch} started.")

            # if epoch > 10:
            #     teacher_forcing = False

            train_loss, train_acc, train_f1 = self.run_epoch(loader_train, is_train=True)
            valid_loss, valid_acc, valid_f1 = self.run_epoch(loader_valid, is_train=False)

            print(f'Train loss: {train_loss}')
            print(f'Train accuracy: {train_acc}')
            print(f'Train (weighted) F1: {train_f1}')

            print(f'Validation loss: {valid_loss}')
            print(f'Validation accuracy: {valid_acc}')
            print(f'Validation (weighted) F1: {valid_f1}')

        # Final result
        test_loss, test_acc, test_f1 = self.run_epoch(loader_test, is_train=False)

        print(f'Test loss: {test_loss}')
        print(f'Test accuracy: {test_acc}')
        print(f'Test (weighted) F1: {test_f1}')

    def run_epoch(self, loader, is_train=False):
        global FLAG
        if is_train:
            FLAG = False
            self.model.train()
        else:
            FLAG = True
            self.model.eval()

        total_loss = 0
        total_samples = 0
        total_steps = 0

        total_preds = []
        total_trues = []

        count = 0

        for batch in tqdm(loader, file=sys.stdout):
            count += 1
            # if not is_train and count > 30:
            #     break

            static_feature, dynamic_feature, labels = batch

            static_feature = static_feature.cuda()
            dynamic_feature = dynamic_feature.cuda()
            labels = labels.cuda()

            logits = self.model(static_feature, dynamic_feature)

            loss = self.criterion(logits, labels)

            total_loss += loss.item()

            if is_train:
                # Calculate grad
                loss.backward()

                self.optimizer.step()
                self.optimizer.zero_grad()

            _, preds = torch.max(logits, 1)

            total_preds.append(preds.detach())
            total_trues.append(labels.detach())

            total_steps += 1
            self.global_steps += 1

        self.lr_scheduler.step()

        preds = torch.cat(total_preds).cpu()
        trues = torch.cat(total_trues).cpu()

        acc = accuracy_score(trues, preds)
        f1 = f1_score(trues, preds, average='weighted')

        return total_loss / total_steps, acc, f1


def run(args):
    torch.manual_seed(7777)
    torch.cuda.set_device(0)
    dataset_train = Dataset('./data/static_train_X.pt', './data/time_train_X.pt', './data/train_y.pt')
    dataset_valid = Dataset('./data/static_val_X.pt', './data/time_val_X.pt', './data/val_y.pt')
    dataset_test = Dataset('./data/static_test_X.pt', './data/time_test_X.pt', './data/test_y.pt')

    trainer = Trainer(args)
    trainer.train(dataset_train, dataset_valid, dataset_test)


args = Hyperparameter(**{
    "num_epochs": 80,
    "batch_size": 128,
    "lr": 0.001,
    "lr_lambda": 0.9999,
    "num_layers": 4,
    "num_heads": 1,
    "head_dim": 4,
    "hidden_dim": 11,
    "dropout_p": 0.2,
    "optimizer": "adam",
    "random_seed": 1313
})

run(args)
